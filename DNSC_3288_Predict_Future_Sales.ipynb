{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3WTlG54jECgm+BvqxYt6o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onealkeegan/DNSC-3288-Predict-Future-Sales/blob/main/DNSC_3288_Predict_Future_Sales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVz_xiF6kM-4"
      },
      "outputs": [],
      "source": [
        "# DNSC-3288-Predict-Future-Sales\n",
        "#Predicting future sales for Fall 2025 Solo Final Project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hmqfQCzk5FQ",
        "outputId": "0a16e0b5-fcd8-45c6-fd63-5555e21d260a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI Use Disclosure: The code below is based heavily on this (https://www.kaggle.com/code/deepdivelm/feature-engineering-lightgbm-exploring-performance) Kaggle dataset. After pre-processing, CHATGPT 5.0 was utilized to help me write more optimized code. ChatGPT 5.0 is reliable at coding, NOT reason based arguments, therefore its' help with coding was able to save me time to focus instead on my model card."
      ],
      "metadata": {
        "id": "mWuyaO2BBPIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Insert Packages & Import Data"
      ],
      "metadata": {
        "id": "j2VsUKGdltTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "import category_encoders as ce\n",
        "import warnings\n",
        "\n",
        "pd.set_option('display.max_rows', 400)\n",
        "pd.set_option('display.max_columns', 160)\n",
        "pd.set_option('display.max_colwidth', 40)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "9isaBOcpkcpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import test data\n",
        "test = pd.read_csv('/kaggle/input/future-sales-data/test.csv')\n",
        "test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "hg6t2yBflC6D",
        "outputId": "d14d4e00-38e0-43fe-a20d-4e774338c24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/future-sales-data/test.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1974097308.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/future-sales-data/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/future-sales-data/test.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Categories"
      ],
      "metadata": {
        "id": "_XunNzp2lxvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering/LightGBM/Exploring Performance - https://www.kaggle.com/code/deepdivelm/feature-engineering-lightgbm-exploring-performance/notebook had english translations"
      ],
      "metadata": {
        "id": "lpgQVImVoNPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = pd.read_csv('/kaggle/input/future-sales-data/categories.csv')\n",
        "categories"
      ],
      "metadata": {
        "id": "WKO5-MImmBrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(categories.category_name.values.reshape(-1, 4))"
      ],
      "metadata": {
        "id": "p163OIg_oRrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Not all category names have a '-' between the main category and the subcategory.\n",
        "#We create groups by extracting the part of the name prior to a non-letter character.\n",
        "#We then create a group_id column by label encoding the group names."
      ],
      "metadata": {
        "id": "u0DyWbPfooeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create broader category groupings\n",
        "categories['group_name'] = categories['category_name'].str.extract(r'(^[\\w\\s]*)')\n",
        "categories['group_name'] = categories['group_name'].str.strip()\n",
        "#label encode group names\n",
        "categories['group_id']  = le.fit_transform(categories.group_name.values)\n",
        "categories.sample(5)"
      ],
      "metadata": {
        "id": "s47ZYbvfotQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Items"
      ],
      "metadata": {
        "id": "BXxUBA38o1pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We load items.csv, clean the name column, create multiple features based on the cleaned name, and use label encoding.\n",
        "#The categories dataframe is then joined to the items dataframe."
      ],
      "metadata": {
        "id": "fIvXKwgto6HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load items\n",
        "items = pd.read_csv('/kaggle/input/future-sales-data/items.csv')\n",
        "items"
      ],
      "metadata": {
        "id": "-O79-JZkpCfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean item_name\n",
        "items['item_name'] = items['item_name'].str.lower()\n",
        "items['item_name'] = items['item_name'].str.replace('.', '')\n",
        "for i in [r'[^\\w\\d\\s\\.]', r'\\bthe\\b', r'\\bin\\b', r'\\bis\\b',\n",
        "          r'\\bfor\\b', r'\\bof\\b', r'\\bon\\b', r'\\band\\b',\n",
        "          r'\\bto\\b', r'\\bwith\\b' , r'\\byo\\b']:\n",
        "    items['item_name'] = items['item_name'].str.replace(i, ' ')\n",
        "items['item_name'] = items['item_name'].str.replace(r'\\b.\\b', ' ')\n"
      ],
      "metadata": {
        "id": "I46HjyGspGIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract first n characters of name\n",
        "items['item_name_no_space'] = items['item_name'].str.replace(' ', '')\n",
        "items['item_name_first4'] = [x[:4] for x in items['item_name_no_space']]\n",
        "items['item_name_first6'] = [x[:6] for x in items['item_name_no_space']]\n",
        "items['item_name_first11'] = [x[:11] for x in items['item_name_no_space']]\n",
        "del items['item_name_no_space']"
      ],
      "metadata": {
        "id": "j_3atpAfpIGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label encode these columns\n",
        "items.item_name_first4 = le.fit_transform(items.item_name_first4.values)\n",
        "items.item_name_first6 = le.fit_transform(items.item_name_first6.values)\n",
        "items.item_name_first11 = le.fit_transform(items.item_name_first11.values)"
      ],
      "metadata": {
        "id": "lSFth80HpJs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#join category_name, group_name and group_id to items\n",
        "items = items.join(categories.set_index('category_id'), on='category_id')\n",
        "items.sample(10)"
      ],
      "metadata": {
        "id": "EFE-aeXVo_lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Duplicate rows exist in the item list.\n",
        "#The following cell creates a dictionary that will allow us to reassign item id's where appropriate."
      ],
      "metadata": {
        "id": "LxR-eehHpVEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dupes = items[(items.duplicated(subset=['item_name','category_id'],keep=False))]\n",
        "dupes['in_test'] = dupes.item_id.isin(test.item_id.unique())\n",
        "dupes = dupes.groupby('item_name').agg({'item_id':['first','last'],'in_test':['first','last']})\n",
        "\n",
        "#if both item id's are in the test set do nothing\n",
        "dupes = dupes[(dupes[('in_test', 'first')]==False) | (dupes[('in_test', 'last')]==False)]\n",
        "#if only the first id is in the test set assign this id to both\n",
        "temp = dupes[dupes[('in_test', 'first')]==True]\n",
        "keep_first = dict(zip(temp[('item_id', 'last')], temp[('item_id',  'first')]))\n",
        "#if neither id or only the second id is in the test set, assign the second id to both\n",
        "temp = dupes[dupes[('in_test', 'first')]==False]\n",
        "keep_second = dict(zip(temp[('item_id', 'first')], temp[('item_id',  'last')]))\n",
        "item_map = {**keep_first, **keep_second}"
      ],
      "metadata": {
        "id": "x-i4_y23pYCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Sales"
      ],
      "metadata": {
        "id": "fIdhu8VzplBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading sales data\n",
        "sales = pd.read_csv('/kaggle/input/future-sales-data/sales_train.csv')"
      ],
      "metadata": {
        "id": "ERjxGBxgpxQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales = (sales\n",
        "    .replace({\n",
        "        'shop_id':{0:57, 1:58, 11:10}, #replacing obsolete shop id's\n",
        "        'item_id':item_map #fixing duplicate item id's\n",
        "    })\n",
        ")"
      ],
      "metadata": {
        "id": "plnD8rw1qDES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing shops which don't appear in the test set\n",
        "sales = sales[sales['shop_id'].isin(test.shop_id.unique())]"
      ],
      "metadata": {
        "id": "mOxEhlZDqK9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make dates in date time format for easy analysis\n",
        "sales['date'] = pd.to_datetime(sales.date,format='%d.%m.%Y')\n",
        "sales['weekday'] = sales.date.dt.dayofweek"
      ],
      "metadata": {
        "id": "KdUQiGlOqMsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first day the item was sold, day 0 is the first day of the training set period\n",
        "sales['first_sale_day'] = sales.date.dt.dayofyear\n",
        "sales['first_sale_day'] += 365 * (sales.date.dt.year-2013)\n",
        "sales['first_sale_day'] = sales.groupby('item_id')['first_sale_day'].transform('min').astype('int16')"
      ],
      "metadata": {
        "id": "vsrWEEyLqSAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#revenue is needed to accurately calculate prices after grouping\n",
        "sales['revenue'] = sales['item_cnt_day']*sales['item_price']"
      ],
      "metadata": {
        "id": "_BcQPnRhpvVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales['revenue']"
      ],
      "metadata": {
        "id": "g9_JiNUbqU2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate the proportion of weekly sales that occurred on each weekday at each shop. Using this information we can assign a measure of weeks of sales power to each month. February always has 4 exactly weeks worth of days since there are no leap years in our time range and all other months have a value >4 since they have extra days of varying sales power.\n",
        "\n",
        "Month, year and first day of the month features are also created."
      ],
      "metadata": {
        "id": "ecNYPv4dqbbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = sales.groupby(['shop_id','weekday']).agg({'item_cnt_day':'sum'}).reset_index()\n",
        "temp = pd.merge(temp, sales.groupby(['shop_id']).agg({'item_cnt_day':'sum'}).reset_index(), on='shop_id', how='left')\n",
        "temp.columns = ['shop_id','weekday', 'shop_day_sales', 'shop_total_sales']\n",
        "temp['day_quality'] = temp['shop_day_sales']/temp['shop_total_sales']\n",
        "temp = temp[['shop_id','weekday','day_quality']]\n",
        "\n",
        "dates = pd.DataFrame(data={'date':pd.date_range(start='2013-01-01',end='2015-11-30')})\n",
        "dates['weekday'] = dates.date.dt.dayofweek\n",
        "dates['month'] = dates.date.dt.month\n",
        "dates['year'] = dates.date.dt.year - 2013\n",
        "dates['date_block_num'] = dates['year']*12 + dates['month'] - 1\n",
        "dates['first_day_of_month'] = dates.date.dt.dayofyear\n",
        "dates['first_day_of_month'] += 365 * dates['year']\n",
        "dates = dates.join(temp.set_index('weekday'), on='weekday')\n",
        "dates = dates.groupby(['date_block_num','shop_id','month','year']).agg({'day_quality':'sum','first_day_of_month':'min'}).reset_index()\n",
        "\n",
        "dates.query('shop_id == 28').head(15)"
      ],
      "metadata": {
        "id": "96getgpCqdyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now group the sales data by month, shop_id a"
      ],
      "metadata": {
        "id": "LO0qWl7IqiRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales = (sales\n",
        "     .groupby(['date_block_num', 'shop_id', 'item_id'])\n",
        "     .agg({\n",
        "         'item_cnt_day':'sum',\n",
        "         'revenue':'sum',\n",
        "         'first_sale_day':'first'\n",
        "     })\n",
        "     .reset_index()\n",
        "     .rename(columns={'item_cnt_day':'item_cnt'})\n",
        ")\n",
        "sales.sample(5)"
      ],
      "metadata": {
        "id": "7AvmwlJyqjzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Construct Training Dataframe"
      ],
      "metadata": {
        "id": "zESs160kuaa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = []\n",
        "for block_num in sales['date_block_num'].unique():\n",
        "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
        "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
        "    df.append(np.array(list(product(*[cur_shops, cur_items, [block_num]]))))\n",
        "\n",
        "df = pd.DataFrame(np.vstack(df), columns=['shop_id', 'item_id', 'date_block_num'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ZLR6MlxeufKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add the appropriate date_block_num value to the test set\n",
        "test['date_block_num'] = 34\n",
        "del test['ID']"
      ],
      "metadata": {
        "id": "DQ9IBN7YugnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#append test set to training dataframe\n",
        "df = pd.concat([df,test]).fillna(0)\n",
        "df = df.reset_index()\n",
        "del df['index']"
      ],
      "metadata": {
        "id": "D4FMjX_EuiEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#join sales and item inforamtion to the training dataframe\n",
        "df = pd.merge(df, sales, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
        "df = pd.merge(df, dates, on=['date_block_num','shop_id'], how='left')\n",
        "df = pd.merge(df, items.drop(columns=['item_name','group_name','category_name']), on='item_id', how='left')"
      ],
      "metadata": {
        "id": "ACmt7UCOujQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Shops"
      ],
      "metadata": {
        "id": "3eeOhKm2t3MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cluster the shops using K-means clustering, using the proportion of their sales they make in each category as features.\n",
        "\n",
        "k=7 was selected because:\n",
        "\n",
        "k=7 resulted in the highest average silhouette score aside from a choice of k=2.\n",
        "k=2 would not provide a useful clustering because it creates a feature with value (shop_id==55)*1.\n",
        "k=7 is also in an appropriate area when using the elbow method"
      ],
      "metadata": {
        "id": "SHQKCu-9umjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading shops.csv\n",
        "shops = pd.read_csv('/kaggle/input/future-sales-data/shops.csv')\n",
        "\n",
        "#clustering shops\n",
        "shops_cats = pd.DataFrame(\n",
        "    np.array(list(product(*[df['shop_id'].unique(), df['category_id'].unique()]))),\n",
        "    columns =['shop_id', 'category_id']\n",
        ")\n",
        "temp = df.groupby(['category_id', 'shop_id']).agg({'item_cnt':'sum'}).reset_index()\n",
        "temp2 = temp.groupby('shop_id').agg({'item_cnt':'sum'}).rename(columns={'item_cnt':'shop_total'})\n",
        "temp = temp.join(temp2, on='shop_id')\n",
        "temp['category_proportion'] = temp['item_cnt']/temp['shop_total']\n",
        "temp = temp[['shop_id', 'category_id', 'category_proportion']]\n",
        "shops_cats = pd.merge(shops_cats, temp, on=['shop_id','category_id'], how='left')\n",
        "shops_cats = shops_cats.fillna(0)\n",
        "\n",
        "shops_cats = shops_cats.pivot(index='shop_id', columns=['category_id'])\n",
        "kmeans = KMeans(n_clusters=7, random_state=0).fit(shops_cats)\n",
        "shops_cats['shop_cluster'] = kmeans.labels_.astype('int8')\n",
        "\n",
        "#adding these clusters to the shops dataframe\n",
        "shops = shops.join(shops_cats['shop_cluster'], on='shop_id')"
      ],
      "metadata": {
        "id": "lKMHeUXet1fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We clean the shop names column then use the opening word to create the shop_city feature. We then create the shop_type feature based on terms that occur in the name of the shop. Both these features are then label encoded."
      ],
      "metadata": {
        "id": "NRtrU4cNt9hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing unused shop ids\n",
        "shops.dropna(inplace=True)\n",
        "\n",
        "#cleaning the name column\n",
        "shops['shop_name'] = shops['shop_name'].str.lower()\n",
        "shops['shop_name'] = shops['shop_name'].str.replace(r'[^\\w\\d\\s]', ' ')\n",
        "\n",
        "#creating a column for the type of shop\n",
        "shops['shop_type'] = 'regular'\n",
        "\n",
        "#there is some overlap in tc and mall, mall is given precedence\n",
        "shops.loc[shops['shop_name'].str.contains(r'tc'), 'shop_type'] = 'tc'\n",
        "shops.loc[shops['shop_name'].str.contains(r'mall|center|mega'), 'shop_type'] = 'mall'\n",
        "shops.loc[shops['shop_id'].isin([9,20]), 'shop_type'] = 'special'\n",
        "shops.loc[shops['shop_id'].isin([12,55]), 'shop_type'] = 'online'\n",
        "\n",
        "#the first word of shop name is largely sufficient as a city feature\n",
        "shops['shop_city'] = shops['shop_name'].str.split().str[0]\n",
        "shops.loc[shops['shop_id'].isin([12,55]), 'shop_city'] = 'online'\n",
        "shops.shop_city = le.fit_transform(shops.shop_city.values)\n",
        "shops.shop_type = le.fit_transform(shops.shop_type.values)\n",
        "shops.head()"
      ],
      "metadata": {
        "id": "IHTj4OfMt_1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add shop information to the training dataframe\n",
        "df = pd.merge(df, shops.drop(columns='shop_name'), on='shop_id', how='left')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ZKfvYajDuCWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step Three:Ages & Aggregating Sales/Price information"
      ],
      "metadata": {
        "id": "1oi1JetBu9Of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a feature showing how many days have passed between the first time an item was sold and the beginning of the current month."
      ],
      "metadata": {
        "id": "7dYd2THWvCCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['first_sale_day'] = df.groupby('item_id')['first_sale_day'].transform('max').astype('int16')\n",
        "df.loc[df['first_sale_day']==0, 'first_sale_day'] = 1035\n",
        "df['prev_days_on_sale'] = [max(idx) for idx in zip(df['first_day_of_month']-df['first_sale_day'],[0]*len(df))]\n",
        "del df['first_day_of_month']"
      ],
      "metadata": {
        "id": "uO8sVtzCvDbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#freeing RAM, removing unneeded columns and encoding object columns\n",
        "del sales, categories, shops, shops_cats, temp, temp2, test, dupes, item_map,\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CPVAcHUpvEyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to clip the target value before aggregating so that mean values are not distorted due to outliers. We retain the unclipped value for use in features that do not aggregate the sales data."
      ],
      "metadata": {
        "id": "rnyO-IbUvGJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['item_cnt_unclipped'] = df['item_cnt']\n",
        "df['item_cnt'] = df['item_cnt'].clip(0, 20)"
      ],
      "metadata": {
        "id": "v-lwDy4CvHaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No columns with float dtype require more than float32 precision and no int dtype columns require values outside the int16 range. The following function will compress the data types of these columns."
      ],
      "metadata": {
        "id": "qZASIIhOvJrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downcast(df):\n",
        "    #reduce size of the dataframe\n",
        "    float_cols = [c for c in df if df[c].dtype in [\"float64\"]]\n",
        "    int_cols = [c for c in df if df[c].dtype in ['int64']]\n",
        "    df[float_cols] = df[float_cols].astype('float32')\n",
        "    df[int_cols] = df[int_cols].astype('int16')\n",
        "    return df\n",
        "df = downcast(df)"
      ],
      "metadata": {
        "id": "6tJSLSqCvK3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These features show how many months have passed since the first appearance of the item/name/category/group/shop"
      ],
      "metadata": {
        "id": "95AGiBGYvMbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['item_age'] = (df['date_block_num'] - df.groupby('item_id')['date_block_num'].transform('min')).astype('int8')\n",
        "df['item_name_first4_age'] = (df['date_block_num'] - df.groupby('item_name_first4')['date_block_num'].transform('min')).astype('int8')\n",
        "df['item_name_first6_age'] = (df['date_block_num'] - df.groupby('item_name_first6')['date_block_num'].transform('min')).astype('int8')\n",
        "df['item_name_first11_age'] = (df['date_block_num'] - df.groupby('item_name_first11')['date_block_num'].transform('min')).astype('int8')\n",
        "df['category_age'] = (df['date_block_num'] - df.groupby('category_id')['date_block_num'].transform('min')).astype('int8')\n",
        "df['group_age'] = (df['date_block_num'] - df.groupby('group_id')['date_block_num'].transform('min')).astype('int8')\n",
        "df['shop_age'] = (df['date_block_num'] - df.groupby('shop_id')['date_block_num'].transform('min')).astype('int8')"
      ],
      "metadata": {
        "id": "dZS-naQLvOJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#indicates whether shops have previously sold the item\n",
        "temp = df.query('item_cnt > 0').groupby(['item_id','shop_id']).agg({'date_block_num':'min'}).reset_index()\n",
        "temp.columns = ['item_id', 'shop_id', 'item_shop_first_sale']\n",
        "df = pd.merge(df, temp, on=['item_id','shop_id'], how='left')\n",
        "df['item_shop_first_sale'] = df['item_shop_first_sale'].fillna(50)\n",
        "#item age that stays at 0 if a shop hasn't sold the item\n",
        "df['item_age_if_shop_sale'] = (df['date_block_num'] > df['item_shop_first_sale']) * df['item_age']\n",
        "#the length of time an item has been for sale without being sold at individual shops\n",
        "df['item_age_without_shop_sale'] = (df['date_block_num'] <= df['item_shop_first_sale']) * df['item_age']\n",
        "del df['item_shop_first_sale']"
      ],
      "metadata": {
        "id": "tDgFrX_pvP9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target variable, 'item_cnt', is the monthly sale count of individual items at individual shops. We now create features showing average monthly sales based on various groupings"
      ],
      "metadata": {
        "id": "KmPjGvGSvRaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agg_cnt_col(df, merging_cols, new_col,aggregation):\n",
        "    temp = df.groupby(merging_cols).agg(aggregation).reset_index()\n",
        "    temp.columns = merging_cols + [new_col]\n",
        "    df = pd.merge(df, temp, on=merging_cols, how='left')\n",
        "    return df\n",
        "\n",
        "#individual items across all shops\n",
        "df = agg_cnt_col(df, ['date_block_num','item_id'],'item_cnt_all_shops',{'item_cnt':'mean'})\n",
        "df = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'item_cnt_all_shops_median',{'item_cnt':'median'})\n",
        "#all items in category at individual shops\n",
        "df = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'category_cnt',{'item_cnt':'mean'})\n",
        "df = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'category_cnt_median',{'item_cnt':'median'})\n",
        "#all items in category across all shops\n",
        "df = agg_cnt_col(df, ['date_block_num','category_id'],'category_cnt_all_shops',{'item_cnt':'mean'})\n",
        "df = agg_cnt_col(df, ['date_block_num','category_id'],'category_cnt_all_shops_median',{'item_cnt':'median'})\n",
        "#all items in group\n",
        "df = agg_cnt_col(df, ['date_block_num','group_id','shop_id'],'group_cnt',{'item_cnt':'mean'})\n",
        "#all items in group across all shops\n",
        "df = agg_cnt_col(df, ['date_block_num','group_id'],'group_cnt_all_shops',{'item_cnt':'mean'})\n",
        "#all items at individual shops\n",
        "df = agg_cnt_col(df, ['date_block_num','shop_id'],'shop_cnt',{'item_cnt':'mean'})\n",
        "#all items at all shops within the city\n",
        "df = agg_cnt_col(df, ['date_block_num','shop_city'],'city_cnt',{'item_cnt':'mean'})"
      ],
      "metadata": {
        "id": "Efm2dLULvT2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now create features showing the mean first month sales for items in each category"
      ],
      "metadata": {
        "id": "fhWX6c_WvVXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_item_sales(df, merging_cols, new_col):\n",
        "    temp = (\n",
        "        df\n",
        "        .query('item_age==0')\n",
        "        .groupby(merging_cols)['item_cnt']\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "        .rename(columns={'item_cnt': new_col})\n",
        "    )\n",
        "    df = pd.merge(df, temp, on=merging_cols, how='left')\n",
        "    return df\n",
        "\n",
        "#mean units sold of new item in category at individual shop\n",
        "df = new_item_sales(df, ['date_block_num','category_id','shop_id'], 'new_items_in_cat')\n",
        "#mean units sold of new item in category across all shops\n",
        "df = new_item_sales(df, ['date_block_num','category_id'], 'new_items_in_cat_all_shops')"
      ],
      "metadata": {
        "id": "Hie66wHYvWqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agg_price_col(df, merging_cols, new_col):\n",
        "    temp = df.groupby(merging_cols).agg({'revenue':'sum','item_cnt_unclipped':'sum'}).reset_index()\n",
        "    temp[new_col] = temp['revenue']/temp['item_cnt_unclipped']\n",
        "    temp = temp[merging_cols + [new_col]]\n",
        "    df = pd.merge(df, temp, on=merging_cols, how='left')\n",
        "    return df\n",
        "\n",
        "#average item price\n",
        "df = agg_price_col(df,['date_block_num','item_id'],'item_price')\n",
        "#average price of items in category\n",
        "df = agg_price_col(df,['date_block_num','category_id'],'category_price')\n",
        "#average price of all items\n",
        "df = agg_price_col(df,['date_block_num'],'block_price')"
      ],
      "metadata": {
        "id": "haAkwXDfvZSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = downcast(df)"
      ],
      "metadata": {
        "id": "fxpMq4L9va__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Lagging Values"
      ],
      "metadata": {
        "id": "Vzr9rwbDvsIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where I change things from https://www.kaggle.com/code/deepdivelm/feature-engineering-lightgbm-exploring-performance\n",
        "\n",
        "The example is not memory efficient, so I will redo it to create all intermediate aggregation columns in groupby and then create all 12-month lag features memory-efficiently"
      ],
      "metadata": {
        "id": "7-topDMQvyby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0️⃣ Sort df by date_block_num\n",
        "df = df.sort_values('date_block_num')\n",
        "\n",
        "# 1️⃣ Aggregate intermediate columns only if they don't already exist\n",
        "if 'item_cnt_all_shops' not in df.columns:\n",
        "    df_item_all = df.groupby(['date_block_num', 'item_id'], as_index=False)['item_cnt'].sum()\n",
        "    df_item_all.rename(columns={'item_cnt':'item_cnt_all_shops'}, inplace=True)\n",
        "    df = df.merge(df_item_all, on=['date_block_num','item_id'], how='left')\n",
        "\n",
        "if 'category_cnt' not in df.columns:\n",
        "    df_cat = df.groupby(['date_block_num', 'shop_id', 'category_id'], as_index=False)['item_cnt'].sum()\n",
        "    df_cat.rename(columns={'item_cnt':'category_cnt'}, inplace=True)\n",
        "    df = df.merge(df_cat, on=['date_block_num','shop_id','category_id'], how='left')\n",
        "\n",
        "if 'category_cnt_all_shops' not in df.columns:\n",
        "    df_cat_all = df.groupby(['date_block_num', 'category_id'], as_index=False)['item_cnt'].sum()\n",
        "    df_cat_all.rename(columns={'item_cnt':'category_cnt_all_shops'}, inplace=True)\n",
        "    df = df.merge(df_cat_all, on=['date_block_num','category_id'], how='left')\n",
        "\n",
        "if 'group_cnt' not in df.columns:\n",
        "    df_group = df.groupby(['date_block_num', 'shop_id', 'group_id'], as_index=False)['item_cnt'].sum()\n",
        "    df_group.rename(columns={'item_cnt':'group_cnt'}, inplace=True)\n",
        "    df = df.merge(df_group, on=['date_block_num','shop_id','group_id'], how='left')\n",
        "\n",
        "if 'group_cnt_all_shops' not in df.columns:\n",
        "    df_group_all = df.groupby(['date_block_num', 'group_id'], as_index=False)['item_cnt'].sum()\n",
        "    df_group_all.rename(columns={'item_cnt':'group_cnt_all_shops'}, inplace=True)\n",
        "    df = df.merge(df_group_all, on=['date_block_num','group_id'], how='left')\n",
        "\n",
        "if 'shop_cnt' not in df.columns:\n",
        "    df_shop = df.groupby(['date_block_num', 'shop_id'], as_index=False)['item_cnt'].sum()\n",
        "    df_shop.rename(columns={'item_cnt':'shop_cnt'}, inplace=True)\n",
        "    df = df.merge(df_shop, on=['date_block_num','shop_id'], how='left')\n",
        "\n",
        "if 'city_cnt' not in df.columns:\n",
        "    df_city = df.groupby(['date_block_num', 'shop_city'], as_index=False)['item_cnt'].sum()\n",
        "    df_city.rename(columns={'item_cnt':'city_cnt'}, inplace=True)\n",
        "    df = df.merge(df_city, on=['date_block_num','shop_city'], how='left')\n",
        "\n",
        "# Handle new items — using new_items_in_cat column\n",
        "if 'new_items_in_cat' not in df.columns:\n",
        "    df_new_cat = df.groupby(['date_block_num', 'shop_id', 'category_id'], as_index=False)['item_cnt'].sum()\n",
        "    df_new_cat.rename(columns={'new_items_in_cat':'new_items_in_cat'}, inplace=True)\n",
        "    df = df.merge(df_new_cat, on=['date_block_num','shop_id','category_id'], how='left')\n",
        "\n",
        "if 'new_items_in_cat_all_shops' not in df.columns:\n",
        "    df_new_cat_all = df.groupby(['date_block_num', 'category_id'], as_index=False)['item_cnt'].sum()\n",
        "    df_new_cat_all.rename(columns={'new_items_in_cat':'new_items_in_cat_all_shops'}, inplace=True)\n",
        "    df = df.merge(df_new_cat_all, on=['date_block_num','category_id'], how='left')\n"
      ],
      "metadata": {
        "id": "Z0kNjELuv0t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create all 12 lag features\n",
        "\n",
        "#per AI use diclosure, the below code was created by Chat GPT. I checked it over for accuracy."
      ],
      "metadata": {
        "id": "WmF3ZvQI0MwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Define lag columns and grouping\n",
        "lag12_cols = {\n",
        "    'item_cnt':['shop_id','item_id'],\n",
        "    'item_cnt_all_shops':['item_id'],\n",
        "    'category_cnt':['shop_id','category_id'],\n",
        "    'category_cnt_all_shops':['category_id'],\n",
        "    'group_cnt':['shop_id','group_id'],\n",
        "    'group_cnt_all_shops':['group_id'],\n",
        "    'shop_cnt':['shop_id'],\n",
        "    'city_cnt':['shop_city'],\n",
        "    'new_items_in_cat':['shop_id','category_id'],\n",
        "    'new_items_in_cat_all_shops':['category_id']\n",
        "}\n",
        "\n",
        "# Columns where we MUST keep lag1 (because later code uses them)\n",
        "keep_lag1 = {\n",
        "    'item_cnt_all_shops',\n",
        "    'category_cnt',\n",
        "    'category_cnt_all_shops'\n",
        "}\n",
        "\n",
        "# 3️⃣ Create 12-month lags memory-efficiently\n",
        "for col, group_cols in lag12_cols.items():\n",
        "    df[f'{col}_lag1to12'] = 0.0  # aggregate column\n",
        "\n",
        "    for lag in range(1, 13):\n",
        "        lag_col = f'{col}_lag{lag}'\n",
        "\n",
        "        # shift\n",
        "        df[lag_col] = (\n",
        "            df.groupby(group_cols)[col]\n",
        "              .shift(lag)\n",
        "              .fillna(0)\n",
        "              .astype('float32')\n",
        "        )\n",
        "\n",
        "        # accumulate\n",
        "        df[f'{col}_lag1to12'] += df[lag_col]\n",
        "\n",
        "        # delete unnecessary lag columns\n",
        "        if (lag == 1 and col in keep_lag1):\n",
        "            pass  # keep lag1 only for selected columns\n",
        "        elif (lag <= 2 and col == 'item_cnt'):\n",
        "            pass  # keep lag1 & lag2 for item_cnt until end of loop\n",
        "        else:\n",
        "            del df[lag_col]\n",
        "\n",
        "    # Delete item_cnt lag1 and lag2 afterward (to match your original design)\n",
        "    if col == 'item_cnt':\n",
        "        del df[f'{col}_lag1']\n",
        "        del df[f'{col}_lag2']\n",
        "    else:\n",
        "        del df[col]  # drop original\n"
      ],
      "metadata": {
        "id": "TDFN00r10Lyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI Use Disclosure: After intitial submission, the model was not performing well. I uploaded my code to Chat GPT with the link to the competiton and asked Chat GPT what my model was missing compared to winning submissions. It recommended adding a rolling-mean, which the top 50 submissions have. This saved me a significant amount of time in comparison to having to individually look at each submission to compare to my own. After, I was also able to ask it to write the code for me based on the .ipynb I had uploaded."
      ],
      "metadata": {
        "id": "XlloWBDeDhtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# NEW: Rolling Means (3, 6, 12 months) for items/categories/shops\n",
        "# -------------------------------\n",
        "\n",
        "# ITEM-LEVEL ROLLING MEANS ----------------------------------------\n",
        "item_matrix = df.pivot_table(\n",
        "    index='item_id',\n",
        "    columns='date_block_num',\n",
        "    values='item_cnt',\n",
        "    aggfunc='sum'\n",
        ").fillna(0)\n",
        "\n",
        "item_rm3 = item_matrix.rolling(window=3, axis=1).mean()\n",
        "item_rm6 = item_matrix.rolling(window=6, axis=1).mean()\n",
        "item_rm12 = item_matrix.rolling(window=12, axis=1).mean()\n",
        "\n",
        "df = df.merge(\n",
        "    item_rm3.stack().reset_index().rename(columns={0:'item_rm3'}),\n",
        "    on=['item_id','date_block_num'], how='left'\n",
        ")\n",
        "df = df.merge(\n",
        "    item_rm6.stack().reset_index().rename(columns={0:'item_rm6'}),\n",
        "    on=['item_id','date_block_num'], how='left'\n",
        ")\n",
        "df = df.merge(\n",
        "    item_rm12.stack().reset_index().rename(columns={0:'item_rm12'}),\n",
        "    on=['item_id','date_block_num'], how='left'\n",
        ")\n",
        "\n",
        "# CATEGORY-LEVEL ROLLING MEANS -------------------------------------\n",
        "cat_matrix = df.pivot_table(\n",
        "    index='category_id',\n",
        "    columns='date_block_num',\n",
        "    values='item_cnt',\n",
        "    aggfunc='sum'\n",
        ").fillna(0)\n",
        "\n",
        "cat_rm3 = cat_matrix.rolling(window=3, axis=1).mean()\n",
        "cat_rm6 = cat_matrix.rolling(window=6, axis=1).mean()\n",
        "cat_rm12 = cat_matrix.rolling(window=12, axis=1).mean()\n",
        "\n",
        "df = df.merge(\n",
        "    cat_rm3.stack().reset_index().rename(columns={0:'category_rm3'}),\n",
        "    on=['category_id','date_block_num'], how='left'\n",
        ")\n",
        "df = df.merge(\n",
        "    cat_rm6.stack().reset_index().rename(columns={0:'category_rm6'}),\n",
        "    on=['category_id','date_block_num'], how='left'\n",
        ")\n",
        "df = df.merge(\n",
        "    cat_rm12.stack().reset_index().rename(columns={0:'category_rm12'}),\n",
        "    on=['category_id','date_block_num'], how='left'\n",
        ")\n",
        "\n",
        "# SHOP-LEVEL ROLLING MEANS ----------------------------------------\n",
        "shop_matrix = df.pivot_table(\n",
        "    index='shop_id',\n",
        "    columns='date_block_num',\n",
        "    values='item_cnt',\n",
        "    aggfunc='sum'\n",
        ").fillna(0)\n",
        "\n",
        "shop_rm3 = shop_matrix.rolling(window=3, axis=1).mean()\n",
        "shop_rm6 = shop_matrix.rolling(window=6, axis=1).mean()\n",
        "shop_rm12 = shop_matrix.rolling(window=12, axis=1).mean()\n",
        "\n",
        "df = df.merge(\n",
        "    shop_rm3.stack().reset_index().rename(columns={0:'shop_rm3'}),\n",
        "    on=['shop_id','date_block_num'], how='left'\n",
        ")\n",
        "df = df.merge(\n",
        "    shop_rm6.stack().reset_index().rename(columns={0:'shop_rm6'}),\n",
        "    on=['shop_id','date_block_num'], how='left'\n",
        ")\n",
        "df = df.merge(\n",
        "    shop_rm12.stack().reset_index().rename(columns={0:'shop_rm12'}),\n",
        "    on=['shop_id','date_block_num'], how='left'\n",
        ")\n",
        "\n",
        "# Downcast new columns to save RAM\n",
        "df = downcast(df)\n"
      ],
      "metadata": {
        "id": "oliaYNvjDhNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take lag1 and lag2 values for these columns"
      ],
      "metadata": {
        "id": "DF0R_Lrqv22q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define columns to create lag1 and lag2 for\n",
        "lag2_cols = {\n",
        "    'item_cnt_unclipped':['shop_id', 'item_id'],\n",
        "    'item_cnt_all_shops_median':['item_id'],\n",
        "    'category_cnt_median':['shop_id', 'category_id'],\n",
        "    'category_cnt_all_shops_median':['category_id']\n",
        "}\n",
        "\n",
        "# Create lag1 and lag2\n",
        "for col, group_cols in lag2_cols.items():\n",
        "    for lag in [1,2]:\n",
        "        lag_col = f'{col}_lag{lag}'\n",
        "        df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0).astype('float32')\n",
        "    # Drop original column if not item_cnt_unclipped\n",
        "    if col != 'item_cnt_unclipped':\n",
        "        del df[col]\n",
        "#clip columns that are near 0\n",
        "bad_cols = [\n",
        "    'item_cnt_diff',\n",
        "    'item_cnt_all_shops_diff',\n",
        "    'category_cnt_diff',\n",
        "    'category_cnt_all_shops_diff'\n",
        "]\n",
        "\n",
        "for c in bad_cols:\n",
        "    if c in df.columns:\n",
        "        df[c] = df[c].replace([np.inf, -np.inf], 0)\n",
        "        df[c] = df[c].fillna(0)\n",
        "        df[c] = df[c].clip(-5, 5)   # keep within a sane range\n",
        "\n",
        "# Compute difference ratios (lag1 / lag1to12) as in original\n",
        "df['item_cnt_diff'] = df['item_cnt_unclipped_lag1'] / df['item_cnt_lag1to12']\n",
        "df['item_cnt_all_shops_diff'] = df['item_cnt_all_shops_median_lag1'] / df['item_cnt_all_shops_lag1to12']\n",
        "df['category_cnt_diff'] = df['category_cnt_median_lag1'] / df['category_cnt_lag1to12']\n",
        "df['category_cnt_all_shops_diff'] = df['category_cnt_all_shops_median_lag1'] / df['category_cnt_all_shops_lag1to12']\n",
        "\n",
        "# Lag category_price and block_price by 1\n",
        "for col, group_cols in {'category_price':['category_id'], 'block_price':[]}.items():\n",
        "    lag_col = f'{col}_lag1'\n",
        "    if group_cols:  # groupby shift\n",
        "        df[lag_col] = df.groupby(group_cols)[col].shift(1).fillna(0).astype('float32')\n",
        "    else:  # shift over full df\n",
        "        df[lag_col] = df[col].shift(1).fillna(0).astype('float32')\n",
        "    del df[col]"
      ],
      "metadata": {
        "id": "DLKxQDq-4bU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilized Chat GPT to optimize the code from the Feature Engineering/LightGBM/Exploring Performance reference by cleaning, normalizing lag features, and creating name & category based time-series features."
      ],
      "metadata": {
        "id": "oL74UVa7Bgxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Fill NaNs for lag1to12 columns where item/category/group age > 0\n",
        "# -------------------------------\n",
        "lag1to12_cols = [\n",
        "    ('item_cnt_lag1to12', ['item_age', 'shop_age']),\n",
        "    ('category_cnt_lag1to12', ['category_age', 'shop_age']),\n",
        "    ('group_cnt_lag1to12', ['group_age', 'shop_age'])\n",
        "]\n",
        "\n",
        "for col, age_cols in lag1to12_cols:\n",
        "    mask = (df[age_cols[0]] > 0) & (df[col].isna())\n",
        "    df.loc[mask, col] = 0\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Normalize lag1to12 columns by minimum age or 12\n",
        "# -------------------------------\n",
        "def normalize_lag(df, col, age_cols):\n",
        "    min_vals = np.minimum.reduce([df[age] for age in age_cols] + [12*np.ones(len(df))])\n",
        "    df[col] = df[col] / min_vals.astype('float32')\n",
        "\n",
        "normalize_lag(df, 'item_cnt_lag1to12', ['item_age','shop_age'])\n",
        "normalize_lag(df, 'item_cnt_all_shops_lag1to12', ['item_age'])\n",
        "normalize_lag(df, 'category_cnt_lag1to12', ['category_age','shop_age'])\n",
        "normalize_lag(df, 'category_cnt_all_shops_lag1to12', ['category_age'])\n",
        "normalize_lag(df, 'group_cnt_lag1to12', ['group_age','shop_age'])\n",
        "normalize_lag(df, 'group_cnt_all_shops_lag1to12', ['group_age'])\n",
        "normalize_lag(df, 'city_cnt_lag1to12', ['date_block_num'])\n",
        "normalize_lag(df, 'shop_cnt_lag1to12', ['shop_age'])\n",
        "normalize_lag(df, 'new_items_in_cat_lag1to12', ['category_age','shop_age'])\n",
        "normalize_lag(df, 'new_items_in_cat_all_shops_lag1to12', ['category_age'])\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Downcast numerical columns once\n",
        "# -------------------------------\n",
        "df = downcast(df)\n",
        "\n",
        "# -------------------------------\n",
        "# 4️⃣ Past information (last price, cumulative sales) — low RAM version\n",
        "# -------------------------------\n",
        "\n",
        "# Make sure data is sorted for time-series operations\n",
        "df = df.sort_values(['item_id', 'date_block_num'])\n",
        "\n",
        "# Last item price before current block\n",
        "df['last_item_price'] = df.groupby('item_id')['item_price'].shift(1)\n",
        "\n",
        "# Cumulative item_cnt by (shop_id, item_id), excluding current row\n",
        "df = df.sort_values(['shop_id', 'item_id', 'date_block_num'])\n",
        "df['item_cnt_sum_alltime'] = (\n",
        "    df.groupby(['shop_id', 'item_id'])['item_cnt']\n",
        "      .cumsum()\n",
        "      .shift(1)\n",
        "      .fillna(0)\n",
        ")\n",
        "\n",
        "# Cumulative item_cnt by item across all shops, excluding current row\n",
        "df = df.sort_values(['item_id', 'date_block_num'])\n",
        "df['item_cnt_sum_alltime_allshops'] = (\n",
        "    df.groupby('item_id')['item_cnt']\n",
        "      .cumsum()\n",
        "      .shift(1)\n",
        "      .fillna(0)\n",
        ")\n",
        "# -------------------------------\n",
        "# 5️⃣ Remove columns causing data leakage\n",
        "# -------------------------------\n",
        "df.drop(['revenue','item_cnt_unclipped','item_price'], axis=1, inplace=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 6️⃣ Compute relative and per-day counts\n",
        "# -------------------------------\n",
        "df['relative_price_item_block_lag1'] = df['last_item_price'] / df['block_price_lag1']\n",
        "df['item_cnt_per_day_alltime'] = (df['item_cnt_sum_alltime'] / df['prev_days_on_sale']).fillna(0)\n",
        "df['item_cnt_per_day_alltime_allshops'] = (df['item_cnt_sum_alltime_allshops'] / df['prev_days_on_sale']).fillna(0)\n",
        "\n",
        "# -------------------------------\n",
        "# 7️⃣ Matching name & category age features — low RAM version\n",
        "# -------------------------------\n",
        "def matching_name_cat_age_fast(df, n, all_shops):\n",
        "    base_cols = [f'item_name_first{n}', 'item_age', 'category_id']\n",
        "    if all_shops:\n",
        "        group_cols = base_cols\n",
        "        target_col = f'same_name{n}catage_cnt_all_shops'\n",
        "    else:\n",
        "        group_cols = base_cols + ['shop_id']\n",
        "        target_col = f'same_name{n}catage_cnt'\n",
        "\n",
        "    # Sort by grouping columns + time\n",
        "    df = df.sort_values(group_cols + ['date_block_num'])\n",
        "\n",
        "    # Group and compute expanding mean of item_cnt, shifted by 1 to avoid leakage\n",
        "    grouped = df.groupby(group_cols)['item_cnt']\n",
        "    hist_mean = grouped.expanding().mean().shift(1)\n",
        "\n",
        "    # hist_mean is a Series with a MultiIndex: (group_cols..., row_index)\n",
        "    df[target_col] = hist_mean.reset_index(level=group_cols, drop=True).fillna(0)\n",
        "\n",
        "    return df\n",
        "\n",
        "for n in [4, 6, 11]:\n",
        "    for all_shops in [True, False]:\n",
        "        df = matching_name_cat_age_fast(df, n, all_shops)\n",
        "# -------------------------------\n",
        "# 8️⃣ Assign appropriate datatypes\n",
        "# -------------------------------\n",
        "int8_cols = [\n",
        "    'item_cnt','month','group_id','shop_type',\n",
        "    'shop_city','shop_id','date_block_num','category_id'\n",
        "]\n",
        "int16_cols = ['item_id','item_name_first4','item_name_first6','item_name_first11']\n",
        "df[int8_cols] = df[int8_cols].astype('int8')\n",
        "df[int16_cols] = df[int16_cols].astype('int16')\n",
        "\n",
        "# -------------------------------\n",
        "# 9️⃣ Nearby item data — low RAM version\n",
        "# -------------------------------\n",
        "\n",
        "def add_nearby_features(df, col, use_shop=True):\n",
        "    if use_shop:\n",
        "        group_keys = ['date_block_num', 'shop_id']\n",
        "    else:\n",
        "        group_keys = ['date_block_num']\n",
        "\n",
        "    # Sort for consistent neighbor definition\n",
        "    df = df.sort_values(group_keys + ['item_id'])\n",
        "\n",
        "    # Below = next item_id's value within group\n",
        "    df[f'below_{col}'] = df.groupby(group_keys)[col].shift(-1)\n",
        "\n",
        "    # Above = previous item_id's value within group\n",
        "    df[f'above_{col}'] = df.groupby(group_keys)[col].shift(1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Only keep columns that actually exist in your current df\n",
        "item_cols = []\n",
        "for c in ['item_cnt_all_shops_lag1', 'item_cnt_all_shops_lag1to12']:\n",
        "    if c in df.columns:\n",
        "        item_cols.append(c)\n",
        "\n",
        "for col in item_cols:\n",
        "    # all_shops versions don't depend on shop_id\n",
        "    df = add_nearby_features(df, col, use_shop=False)\n",
        "\n",
        "# -------------------------------\n",
        "# 10️⃣ Final downcast & garbage collection\n",
        "# -------------------------------\n",
        "df = downcast(df)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "P0LG1_3H46Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizations completed:\n",
        "\n",
        "Vectorized NaN filling and lag1to12 normalization using np.minimum.reduce instead of row-wise zip.\n",
        "\n",
        "Removed unnecessary del temp statements.\n",
        "\n",
        "Avoided repeated .query() inside loops wherever possible.\n",
        "\n",
        "Downcast only once at the end.\n",
        "\n",
        "All functions now return merged DataFrames cleanly.\n",
        "\n",
        "Memory-efficient, fewer merges, should run faster and use less RAM than original code."
      ],
      "metadata": {
        "id": "u-vhTACF5Em8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Modeling"
      ],
      "metadata": {
        "id": "wV1_CSpH3nek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import shap\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "BMCKS4zI4ipu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df[~df.date_block_num.isin([0,1,33,34])]\n",
        "y_train = X_train['item_cnt']\n",
        "del X_train['item_cnt']\n",
        "\n",
        "X_val = df[df['date_block_num']==33]\n",
        "y_val = X_val['item_cnt']\n",
        "del X_val['item_cnt']\n",
        "\n",
        "X_test = df[df['date_block_num']==34].drop(columns='item_cnt')\n",
        "X_test = X_test.reset_index()\n",
        "del X_test['index']\n",
        "\n",
        "#free memory\n",
        "del df"
      ],
      "metadata": {
        "id": "rGBVJ2k632k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features):\n",
        "\n",
        "    lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=cat_features)\n",
        "    lgb_val = lgb.Dataset(X_val, y_val, categorical_feature=cat_features)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params=params,\n",
        "        train_set=lgb_train,\n",
        "        num_boost_round=params.get(\"num_boost_round\", 300),\n",
        "        valid_sets=[lgb_val]\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "GpsjDOdU4SJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'num_leaves': 60,\n",
        "    'min_data_in_leaf':50,\n",
        "    'feature_fraction':0.7,\n",
        "    'learning_rate': 0.01,\n",
        "    'num_rounds': 500,\n",
        "    \"early_stopping_round\":50,\n",
        "    'seed': 1,\n",
        "    'verbosity':-1\n",
        "}\n",
        "#designating the categorical features which should be focused on\n",
        "cat_features = ['category_id','month','shop_id','shop_city']\n",
        "\n",
        "lgb_model = build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features)\n",
        "\n",
        "#save model for later use\n",
        "lgb_model.save_model('initial_lgb_model.txt')"
      ],
      "metadata": {
        "id": "KPt2A9uM4bYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model Analysis"
      ],
      "metadata": {
        "id": "RHHG-R_Y6B3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add columns showing predicted values, along with target values and sq_error for our training and validation sets"
      ],
      "metadata": {
        "id": "GY9BD2zU6J7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['lgb_pred'] = lgb_model.predict(X_train).clip(0,20)\n",
        "X_train['target'] = y_train\n",
        "X_train['sq_err'] = (X_train['lgb_pred']-X_train['target'])**2\n",
        "\n",
        "X_val['lgb_pred'] = lgb_model.predict(X_val).clip(0,20)\n",
        "X_val['target'] = y_val\n",
        "X_val['sq_err'] = (X_val['lgb_pred']-X_val['target'])**2\n",
        "\n",
        "X_test['lgb_pred'] = lgb_model.predict(X_test).clip(0,20)"
      ],
      "metadata": {
        "id": "qKwcGumr6HTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "rmse = mean_squared_error(y_val, X_val['lgb_pred'], squared=False)\n",
        "print(\"Validation RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "F2pE6mBaHtuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a dataframe that will allow us to easily graph some aspects of model's performance."
      ],
      "metadata": {
        "id": "vRhPU4m86Mam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = X_train.groupby('date_block_num').agg({'lgb_pred':'mean','target':'mean','sq_err':'mean'}).reset_index()\n",
        "data['new_item_rmse'] = np.sqrt(X_train.query('item_age<=1').groupby('date_block_num').agg({'sq_err':'mean'}).sq_err)\n",
        "data['old_item_rmse'] = np.sqrt(X_train.query('item_age>1').groupby('date_block_num').agg({'sq_err':'mean'}).sq_err)\n",
        "data = pd.concat([\n",
        "    data,\n",
        "    pd.DataFrame([\n",
        "        {'date_block_num':33,\n",
        "         'target':X_val.target.mean(),\n",
        "         'lgb_pred':X_val.lgb_pred.mean(),\n",
        "         'sq_err':np.sqrt(X_val.sq_err.mean()),\n",
        "         'old_item_rmse':np.sqrt(X_val.query('item_age>1').sq_err.mean()),\n",
        "         'new_item_rmse':np.sqrt(X_val.query('item_age<=1').sq_err.mean())},\n",
        "        {'date_block_num':34,\n",
        "         'target':0,\n",
        "         'lgb_pred':X_test.lgb_pred.mean(),\n",
        "         'sq_err':0,\n",
        "         'old_item_rmse':0,\n",
        "         'new_item_rmse':0}\n",
        "    ])\n",
        "], ignore_index=True)\n",
        "\n",
        "data['date'] = [x[:7] for x in pd.date_range(start='2013-03',end='2015-09',freq='MS').astype('str')]+['Validation','Test']"
      ],
      "metadata": {
        "id": "Ku6_AW0K6PV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A summary plot of feature importance. We can use this along with other shap plots to get an idea of what drives the models predictions, and help us find spurious features which can be filtered out."
      ],
      "metadata": {
        "id": "bkiKzakz6SMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = X_test.drop(columns='lgb_pred').sample(10000)\n",
        "explainer = shap.TreeExplainer(lgb_model)\n",
        "shap_values = explainer.shap_values(temp)\n",
        "shap.summary_plot(shap_values, temp, max_display=30)"
      ],
      "metadata": {
        "id": "XKafgmmg6TuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a chart of predicted vs. actual values"
      ],
      "metadata": {
        "id": "cMznfwXt6clv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Prediction',x=data.date,y=data.lgb_pred),\n",
        "    go.Bar(name='Target',x=data.date,y=data.target)\n",
        "])\n",
        "fig.update_layout(\n",
        "    title='Mean Prediction and Target Values by Month',\n",
        "    xaxis={'title':'Month','type':'category'},\n",
        "    yaxis={'title':'Mean Value'},\n",
        "    legend={'yanchor':'top','y':1.05,'xanchor':'left','x':0.01},\n",
        "    template='plotly_dark'\n",
        ")\n",
        "\n",
        "text = '''\n",
        "    The model may not be adequately accounting for<br>\n",
        "    the December sales spike. There is no systemic under or<br>\n",
        "    over-prediction visible for the test month, November.\n",
        "'''\n",
        "fig.add_annotation(\n",
        "    yref='paper', y=1.1,\n",
        "    xref='paper', x=0.7,\n",
        "    text=text,\n",
        "    font={'size':11},\n",
        "    showarrow=False)\n",
        "\n",
        "text = '''\n",
        "    Prediction mean is very close to the  <br>\n",
        "    target mean in our validation set.\n",
        "'''\n",
        "fig.add_annotation(\n",
        "    xref='paper', x=0.95,\n",
        "    yref='paper', y=0.58,\n",
        "    text=text,\n",
        "    font={'size':11},\n",
        "    showarrow=True, arrowhead=1)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5Qm1bQqm6b2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding my submission code"
      ],
      "metadata": {
        "id": "LsA5O0c67gOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_submit = pd.read_csv('/kaggle/input/future-sales-data/test.csv')\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": test_submit[\"ID\"],\n",
        "    \"item_cnt_month\": X_test[\"lgb_pred\"]\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "submission.head()"
      ],
      "metadata": {
        "id": "CWaXX_ju7foZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding RMSE\n",
        "\n",
        "CHAT GPT 5.0 helped me write the code below."
      ],
      "metadata": {
        "id": "ZSMe4HfAtoel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eVXaVbCb17D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remake features so they are matched for RMSE, as more columns were added during analysis"
      ],
      "metadata": {
        "id": "OMLYNuoz2Nim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = set(X_train.columns)\n",
        "model_features = set(lgb_model.feature_name())\n",
        "\n",
        "extra_in_X = train_features - model_features\n",
        "missing_in_X = model_features - train_features\n",
        "\n",
        "print(\"Extra in X_train (should be empty):\", extra_in_X)\n",
        "print(\"Missing from X_train (should be empty):\", missing_in_X)"
      ],
      "metadata": {
        "id": "bblSAIj918j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_features = lgb_model.feature_name()\n",
        "\n",
        "X_train = X_train[final_features]\n",
        "X_val   = X_val[final_features]\n",
        "X_test  = X_test[final_features]"
      ],
      "metadata": {
        "id": "sp4HCf0X19tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = lgb_model.predict(X_train).clip(0, 20)\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "\n",
        "y_pred_val = lgb_model.predict(X_val).clip(0, 20)\n",
        "rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n"
      ],
      "metadata": {
        "id": "zIpAQ4KG1_AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST PREDICTIONS ONLY (NO RMSE POSSIBLE)\n",
        "# -----------------------\n",
        "y_pred_test = lgb_model.predict(X_test).clip(0, 20)\n",
        "y_pred_test"
      ],
      "metadata": {
        "id": "6pNQzYO_2AH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------\n",
        "# OUTPUT\n",
        "# -----------------------\n",
        "print(\"RMSE RESULTS\")\n",
        "print(\"-\" * 35)\n",
        "print(f\"Train RMSE:      {rmse_train:.4f}\")\n",
        "print(f\"Validation RMSE: {rmse_val:.4f}\")\n",
        "print(\"Test RMSE:       Not available (no y_test)\")"
      ],
      "metadata": {
        "id": "M20LP8FxH6a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_df = pd.DataFrame({\n",
        "    \"Dataset\": [\"Train\", \"Validation\"],\n",
        "    \"RMSE\": [rmse_train, rmse_val]\n",
        "})\n",
        "\n",
        "rmse_df"
      ],
      "metadata": {
        "id": "fu9VUI1l2DHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count # of Rows for Model Card"
      ],
      "metadata": {
        "id": "34vZIsZsIE0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = X_train.shape[0]\n",
        "n_val   = X_val.shape[0]\n",
        "n_test   = X_test.shape[0]\n",
        "print(f\"Training rows:   {n_train:,}\")\n",
        "print(f\"Validation rows: {n_val:,}\")\n",
        "print(f\"Test rows: {n_test:,}\")"
      ],
      "metadata": {
        "id": "TbrLlwHuIEjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Target row checks:\")\n",
        "print(\"y_train:\", y_train.shape[0])\n",
        "print(\"y_val:  \", y_val.shape[0])"
      ],
      "metadata": {
        "id": "NzpYmwwt2FiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per Month RMSE"
      ],
      "metadata": {
        "id": "cj77sC202G6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = lgb_model.predict(X_train).clip(0, 20)\n",
        "\n",
        "train_eval = X_train.copy()\n",
        "train_eval[\"y_true\"] = y_train.values\n",
        "train_eval[\"y_pred\"] = y_pred_train\n",
        "\n",
        "rmse_by_month_train = (\n",
        "    train_eval\n",
        "    .groupby(\"date_block_num\")\n",
        "    .apply(lambda x: np.sqrt(mean_squared_error(x[\"y_true\"], x[\"y_pred\"])))\n",
        "    .reset_index(name=\"RMSE\")\n",
        ")\n",
        "\n",
        "print(rmse_by_month_train)\n"
      ],
      "metadata": {
        "id": "qKe3pCuR2MAf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}